{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SPACY_RobertaModel_NER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy[transformers]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm6CwaspVbP-",
        "outputId": "26728db8-1afd-4175-8ac9-13568f62018d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy[transformers] in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.21.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.4.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.7.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.0.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (4.64.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.9.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (8.0.17)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.3.0)\n",
            "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.1.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy[transformers]) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[transformers]) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[transformers]) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2.10)\n",
            "Requirement already satisfied: transformers<4.20.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (4.19.4)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.8.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (1.11.0+cu113)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.20.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (3.7.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy[transformers]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[transformers]) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhFYyFe8v0GI",
        "outputId": "52d1e87c-601b-452e-9033-04aec20e31b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import spacy\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from spacy.tokens import DocBin"
      ],
      "metadata": {
        "id": "2J3QLnz4wJ3Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"/content/gdrive/MyDrive/ResumeRanker\""
      ],
      "metadata": {
        "id": "RNbaJjJlv-dT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data_from_json_spacy3(filepath):\n",
        "    text_dataset = []\n",
        "    dataset = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in tqdm(lines,desc='Extracting Data    '):\n",
        "        data = json.loads(line)\n",
        "        text = data['content'].replace(\"\\n\", \" \")\n",
        "        data_annotations = data['annotation']\n",
        "        entities = []\n",
        "        if data_annotations is not None:\n",
        "            for annotation in data_annotations:\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                if isinstance(labels, list):\n",
        "                    if not labels:\n",
        "                        continue\n",
        "                    label = labels[0]\n",
        "                else:\n",
        "                    label = labels\n",
        "\n",
        "                point_start = point['start']\n",
        "                point_end = point['end']\n",
        "                point_text = point['text']\n",
        "                \n",
        "                lspace = len(point_text) - len(point_text.lstrip())\n",
        "                rspace = len(point_text) - len(point_text.rstrip())\n",
        "                if lspace != 0:\n",
        "                    point_start = point_start + lspace\n",
        "                if rspace != 0:\n",
        "                    point_end = point_end - rspace\n",
        "                entities.append((point_start, point_end + 1 , label))\n",
        "        dataset.append((text, {\"entities\" : entities}))\n",
        "        text_dataset.append(text)\n",
        "\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in tqdm(dataset,desc='Processing Entities'):\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start > 0 and valid_start < len(text):\n",
        "                if invalid_span_tokens.match(text[valid_start]):\n",
        "                    valid_start += 1\n",
        "                elif (not invalid_span_tokens.match(text[valid_start])) and (not invalid_span_tokens.match(text[valid_start-1])):\n",
        "                    valid_start -= 1\n",
        "                else:\n",
        "                    break\n",
        "            while valid_end > 1 and valid_end < len(text):\n",
        "                if invalid_span_tokens.match(text[valid_end - 1]):\n",
        "                    valid_end -= 1\n",
        "                elif (not invalid_span_tokens.match(text[valid_end-1])) and (not invalid_span_tokens.match(text[valid_end])):\n",
        "                    valid_end += 1\n",
        "                else:\n",
        "                    break\n",
        "            valid_entities.append((valid_start, valid_end, label))\n",
        "        cleaned_data.append({'text':text,'entities': valid_entities})\n",
        "    return cleaned_data"
      ],
      "metadata": {
        "id": "5GadGu8Yv-zs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = \"/content/gdrive/MyDrive/ResumeRanker/Dataset/Entity Recognition in Resumes.json\"\n",
        "training_data = extract_data_from_json_spacy3(json_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbEqBverv-2S",
        "outputId": "b3f7f1b7-92dc-4769-8f1f-58930382d58b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Data    : 100%|██████████| 220/220 [00:00<00:00, 10334.04it/s]\n",
            "Processing Entities: 100%|██████████| 220/220 [00:00<00:00, 12040.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\") # load a new spacy model\n",
        "doc_bin = DocBin()"
      ],
      "metadata": {
        "id": "UxbPrGemWGGp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for training_example  in tqdm(training_data): \n",
        "    text = training_example['text']\n",
        "    labels = training_example['entities']\n",
        "    doc = nlp.make_doc(text) \n",
        "    ents = []\n",
        "    for start, end, label in labels:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        if span is None:\n",
        "            print(\"Skipping entity\")\n",
        "        else:\n",
        "            ents.append(span)\n",
        "    filtered_ents = filter_spans(ents)\n",
        "    doc.ents = filtered_ents \n",
        "    doc_bin.add(doc)\n",
        "\n",
        "doc_bin.to_disk(\"/content/gdrive/MyDrive/ResumeRanker/Models/SPACY3_NER/training_data.spacy\") # save the docbin object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsf0ma-zWOiw",
        "outputId": "0860e38e-c74a-47e6-a418-6fd918fe0024"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████▏  | 157/220 [00:01<00:00, 127.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 220/220 [00:02<00:00, 105.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/ResumeRanker/Models/SPACY3_NER"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjg6gXoJWjrt",
        "outputId": "883738f2-13e0-46d8-b789-3c59de479f4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ResumeRanker/Models/SPACY3_NER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ikc0b-0X_om",
        "outputId": "25773457-820e-4c92-b17c-2241311d9bd8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKch0v5AYuwl",
        "outputId": "f0a51cfe-9db2-4f7f-8591-699393ab12b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-06-20 09:24:32,070] [INFO] Set up nlp object from config\n",
            "[2022-06-20 09:24:32,079] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2022-06-20 09:24:32,083] [INFO] Created vocabulary\n",
            "[2022-06-20 09:24:32,084] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-06-20 09:24:46,707] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        1592.65   1593.75    0.16    0.08    3.02    0.00\n",
            "  2     200      181187.46  72300.85   19.21   14.94   26.87    0.19\n",
            "  5     400       49211.41  35871.16   31.48   23.53   47.55    0.31\n",
            "  7     600       16010.59  29196.59   38.60   27.70   63.67    0.39\n",
            " 10     800        9840.94  28871.92   45.44   34.04   68.34    0.45\n",
            " 12    1000        3646.35  25766.51   49.83   35.71   82.38    0.50\n",
            " 15    1200       14131.64  24727.02   83.16   81.87   84.49    0.83\n",
            " 17    1400       18386.93  24311.06   87.88   87.10   88.66    0.88\n",
            " 20    1600       14336.01  21585.76   78.51   76.83   80.26    0.79\n",
            " 23    1800       10763.39  22267.05   88.36   87.44   89.31    0.88\n",
            " 25    2000        1598.59  19267.97   86.25   81.10   92.10    0.86\n",
            " 28    2200        1284.42  19680.89   91.29   93.91   88.81    0.91\n",
            " 30    2400        1758.93  17654.01   88.31   84.59   92.36    0.88\n",
            " 33    2600        1643.25  16873.27   92.01   95.11   89.10    0.92\n",
            " 35    2800        1072.81  15571.24   93.52   93.82   93.22    0.94\n",
            " 38    3000         711.24  13563.28   94.54   96.63   92.54    0.95\n",
            " 41    3200       12609.75  12809.91   94.16   95.03   93.30    0.94\n",
            " 43    3400        7656.75   9989.18   94.71   95.25   94.19    0.95\n",
            " 46    3600        1569.60   8651.02   95.31   96.62   94.04    0.95\n",
            " 48    3800        9577.14   6646.54   93.90   93.13   94.68    0.94\n",
            " 51    4000        2497.74   5204.19   95.50   96.63   94.39    0.95\n",
            " 53    4200        1218.93   3746.22   95.22   95.29   95.15    0.95\n",
            " 56    4400         503.84   2555.47   95.29   96.45   94.16    0.95\n",
            " 58    4600         613.33   1927.21   95.17   94.89   95.45    0.95\n",
            " 61    4800         450.08   1187.65   95.85   97.11   94.63    0.96\n",
            " 64    5000         605.40    930.10   95.10   95.39   94.80    0.95\n",
            " 66    5200         489.38    680.69   95.70   96.56   94.86    0.96\n",
            " 69    5400         490.28    528.21   95.22   96.28   94.19    0.95\n",
            " 71    5600         737.35    536.58   95.04   95.17   94.92    0.95\n",
            " 74    5800         708.65    428.44   95.22   96.53   93.95    0.95\n",
            " 76    6000         913.68    529.74   95.77   95.80   95.74    0.96\n",
            " 79    6200         548.79    401.95   95.75   97.24   94.30    0.96\n",
            " 82    6400         731.20    416.51   95.39   95.66   95.12    0.95\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"/content/gdrive/MyDrive/ResumeRanker/Models/SPACY3_NER/model-best\")"
      ],
      "metadata": {
        "id": "p30JB6fDZJwq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in training_data[:2]:\n",
        "    text = i[\"text\"]\n",
        "    print(\"Data :\")\n",
        "    print(text)\n",
        "    doc = nlp(\" \".join(text.split('\\n')))\n",
        "    for ent in doc.ents:\n",
        "        print(f'{ent.label_.upper():{20}} - {ent.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0e9rfA9_4pT",
        "outputId": "0cacab00-6282-487b-a58c-e85c193d9e4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\n",
            "NAME                 - Abhishek Jha\n",
            "DESIGNATION          - Application Development Associate\n",
            "COMPANIES WORKED AT  - Accenture\n",
            "LOCATION             - Bengaluru,\n",
            "EMAIL ADDRESS        - Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\n",
            "DESIGNATION          - Application Development Associate\n",
            "COMPANIES WORKED AT  - Accenture\n",
            "DEGREE               - B.E in Information science and engineering\n",
            "DEGREE               - B.v.b college of engineering and technology\n",
            "GRADUATION YEAR      - 2017\n",
            "COLLEGE NAME         - Woodbine modern school\n",
            "COLLEGE NAME         - Kendriya Vidyalaya\n",
            "SKILLS               - C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)\n",
            "Data :\n",
            "Afreen Jamadar Active member of IIIT Committee in Third year  Sangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6  I wish to use my knowledge, skills and conceptual understanding to create excellent team environments and work consistently achieving organization objectives believes in taking initiative and work to excellence in my work.  WORK EXPERIENCE  Active member of IIIT Committee in Third year  Cisco Networking -  Kanpur, Uttar Pradesh  organized by Techkriti IIT Kanpur and Azure Skynet. PERSONALLITY TRAITS: • Quick learning ability • hard working  EDUCATION  PG-DAC  CDAC ACTS  2017  Bachelor of Engg in Information Technology  Shivaji University Kolhapur -  Kolhapur, Maharashtra  2016  SKILLS  Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)  ADDITIONAL INFORMATION  TECHNICAL SKILLS:  • Programming Languages: C, C++, Java, .net, php. • Web Designing: HTML, XML • Operating Systems: Windows […] Windows Server 2003, Linux. • Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.  https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\n",
            "NAME                 - Afreen Jamadar\n",
            "LOCATION             - Sangli,\n",
            "EMAIL ADDRESS        - indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\n",
            "DEGREE               - PG-DAC\n",
            "COLLEGE NAME         - CDAC ACTS\n",
            "GRADUATION YEAR      - 2017\n",
            "DEGREE               - Bachelor of Engg in Information Technology\n",
            "COLLEGE NAME         - Shivaji University Kolhapur\n",
            "GRADUATION YEAR      - 2016\n",
            "SKILLS               - Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)  ADDITIONAL INFORMATION  TECHNICAL SKILLS:  • Programming Languages: C, C++, Java, .net, php. • Web Designing: HTML, XML • Operating Systems: Windows […] Windows Server 2003, Linux. • Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\n",
            "EMAIL ADDRESS        - https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "Bhanu Prakash Pebbeti \n",
        "\n",
        "ML/DL Enthusiast | Implementation based learner | Looking for an opportunity to expand my\n",
        "learning, knowledge and skills which help me in achieving greater practical excellence and\n",
        "contribute to the success of the organization. \n",
        "\n",
        "pebbetibhanu2017@gmail.com \n",
        "\n",
        "+91 6303733897 \n",
        "\n",
        "Hyderabad, Telangana, India \n",
        "\n",
        "www.hackerrank.com/bhanuprakash_b12 \n",
        "\n",
        "linkedin.com/in/bhanu-prakash-pebbeti-700b80191 \n",
        "\n",
        "github.com/BhanuPrakashPebbeti \n",
        "\n",
        "EDUCATION \n",
        "\n",
        "ELECTRONICS AND COMMUNICATION\n",
        "ENGINEERING | B.TECH \n",
        "National Institute of Technology Calicut \n",
        "2019 - Present,  \n",
        "\n",
        "CGPA-8.72/10(till 5th sem) \n",
        "\n",
        "SKILLS \n",
        "\n",
        "Python \n",
        "\n",
        "ML \n",
        "\n",
        "AI \n",
        "\n",
        "DL \n",
        "\n",
        "WORK EXPERIENCE \n",
        "\n",
        "INTERMEDIATE \n",
        "Narayana Junior College,Hyderabad \n",
        "2017 - 2019,  \n",
        "\n",
        "Percentage-97.7% \n",
        "\n",
        "Member at AI Club NITC (11/2020 - Present)\n",
        "One of the member at AI Club NITC, aimed at high quality\n",
        "Artiﬁcial Intelligence research and developing Artiﬁcial\n",
        "Intelligence systems for real world applications. \n",
        "\n",
        "SECONDARY HIGH SCHOOL-SSC \n",
        "Shivappa High School,Hyderabad \n",
        "2017,  \n",
        "\n",
        "GPA-9.5/10 \n",
        "\n",
        "Computer Vision Engineer at Intelligent\n",
        "Mobility Labs (06/2021 - Present) \n",
        "Research Lab focused on Self Driving Technology and\n",
        "Autonomous Mobile Robots. \n",
        "\n",
        "PROJECTS \n",
        "\n",
        "Automation  of  Cleaning  Cervical  dataset  using  deep\n",
        "learning techniques (01/2021 - 05/2021) \n",
        "\n",
        "Used Supervised contrastive learning to remove outliers and boost\n",
        "our classiﬁer performance. \n",
        "\n",
        "Multi Task Learning(MTL) for Self Driving Technology\n",
        " (05/2021 - Present)\n",
        "\n",
        "Worked on Perception stack for Indian Road Conditions which\n",
        "includes Semantic segmentation, Depth Estimation and Object\n",
        "detection using MTL. \n",
        "\n",
        "Reinforcement Learning to solve Games\n",
        "\n",
        "Worked on models like Reinforce, Sarsa, Q-Learning, DQN, Deuling\n",
        "DQN to solve games like Balancing Pendulum, CartPole, Lunar\n",
        "Lander from OpenAI Gym and custom made environments like Flappy\n",
        "Bird. \n",
        "\n",
        "Image Generation using VQVAE\n",
        "\n",
        "Used VQVAE to learn discreate representations of the images and\n",
        "then a gpt prior is trained on top of these representations to\n",
        "generate new images. \n",
        "\n",
        "CERTIFICATIONS \n",
        "\n",
        "Applied Data Science With Python\n",
        "Specialization (08/2020)\n",
        "Coursera-University of Michigan \n",
        "\n",
        "Neural Networks and Deep Learning (08/2020)\n",
        "\n",
        "Coursera-deeplearning.ai \n",
        "\n",
        "Python for Everybody Specialization (05/2020)\n",
        "\n",
        "Coursera-University of Michigan \n",
        "\n",
        "LANGUAGES \n",
        "\n",
        "English \n",
        "Fluent \n",
        "\n",
        "Telugu \n",
        "Native \n",
        "\n",
        "Sudoko Solver\n",
        "\n",
        "Application made using python which solves sudoko puzzles with a\n",
        "simple Graphical user interface made using pygame. \n",
        "\n",
        "INTERESTS \n",
        "\n",
        "Reading blogs \n",
        "\n",
        "Playing Sports(cricket) \n",
        "'''"
      ],
      "metadata": {
        "id": "tqn6yj3-B-XD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\" \".join(text.split('\\n')))\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_.upper():{20}} - {ent.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO2jh9cNBvyx",
        "outputId": "1e0c2a86-70eb-485c-bd6c-fd0a45497fc6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                 - Bhanu Prakash Pebbeti\n",
            "COLLEGE NAME         - National Institute of Technology Calicut\n",
            "SKILLS               - Python   ML   AI   DL\n",
            "COLLEGE NAME         - Narayana Junior College,Hyderabad\n",
            "COLLEGE NAME         - Coursera-University of Michigan\n",
            "COLLEGE NAME         - Coursera-University of Michigan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Amfp_g6PCBk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
