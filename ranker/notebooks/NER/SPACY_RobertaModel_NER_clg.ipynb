{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SPACY_RobertaModel_NER_clg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy[transformers]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm6CwaspVbP-",
        "outputId": "f291bcf1-f665-4cc8-df66-62f859787806"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy[transformers] in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.4.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.7.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.4.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (8.0.17)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (4.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.9.1)\n",
            "Collecting spacy-transformers<1.2.0,>=1.1.2\n",
            "  Downloading spacy_transformers-1.1.7-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy[transformers]) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[transformers]) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[transformers]) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (3.0.4)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.8.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 10.1 MB/s \n",
            "\u001b[?25hCollecting transformers<4.21.0,>=3.4.0\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (1.12.0+cu113)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (3.7.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy[transformers]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[transformers]) (2.0.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, spacy-alignments, spacy-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 spacy-alignments-0.8.5 spacy-transformers-1.1.7 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhFYyFe8v0GI",
        "outputId": "bc3c643f-3900-4290-f45e-87f0b23187f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import spacy\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.util import filter_spans"
      ],
      "metadata": {
        "id": "2J3QLnz4wJ3Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"/content/gdrive/MyDrive/ResumeRanker\""
      ],
      "metadata": {
        "id": "RNbaJjJlv-dT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data_from_json_spacy3(filepath):\n",
        "    text_dataset = []\n",
        "    dataset = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in tqdm(lines,desc='Extracting Data    '):\n",
        "        data = json.loads(line)\n",
        "        text = data['content'].replace(\"\\n\", \" \")\n",
        "        data_annotations = data['annotation']\n",
        "        entities = []\n",
        "        if data_annotations is not None:\n",
        "            for annotation in data_annotations:\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                if isinstance(labels, list):\n",
        "                    if not labels:\n",
        "                        continue\n",
        "                    label = labels[0]\n",
        "                else:\n",
        "                    label = labels\n",
        "                if (label == \"College Name\") or (label == \"Degree\"):\n",
        "                    point_start = point['start']\n",
        "                    point_end = point['end']\n",
        "                    point_text = point['text']\n",
        "                    \n",
        "                    lspace = len(point_text) - len(point_text.lstrip())\n",
        "                    rspace = len(point_text) - len(point_text.rstrip())\n",
        "                    if lspace != 0:\n",
        "                        point_start = point_start + lspace\n",
        "                    if rspace != 0:\n",
        "                        point_end = point_end - rspace\n",
        "                    entities.append((point_start, point_end + 1 , label))\n",
        "        dataset.append((text, {\"entities\" : entities}))\n",
        "        text_dataset.append(text)\n",
        "\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in tqdm(dataset,desc='Processing Entities'):\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start > 0 and valid_start < len(text):\n",
        "                if invalid_span_tokens.match(text[valid_start]):\n",
        "                    valid_start += 1\n",
        "                elif (not invalid_span_tokens.match(text[valid_start])) and (not invalid_span_tokens.match(text[valid_start-1])):\n",
        "                    valid_start -= 1\n",
        "                else:\n",
        "                    break\n",
        "            while valid_end > 1 and valid_end < len(text):\n",
        "                if invalid_span_tokens.match(text[valid_end - 1]):\n",
        "                    valid_end -= 1\n",
        "                elif (not invalid_span_tokens.match(text[valid_end-1])) and (not invalid_span_tokens.match(text[valid_end])):\n",
        "                    valid_end += 1\n",
        "                else:\n",
        "                    break\n",
        "            valid_entities.append((valid_start, valid_end, label))\n",
        "        cleaned_data.append({'text':text,'entities': valid_entities})\n",
        "    return cleaned_data"
      ],
      "metadata": {
        "id": "5GadGu8Yv-zs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = \"/content/gdrive/MyDrive/ResumeRanker/Dataset/Entity Recognition in Resumes.json\"\n",
        "training_data = extract_data_from_json_spacy3(json_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbEqBverv-2S",
        "outputId": "1ea761d7-ede1-45d3-ef52-48bae537ef11"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Data    : 100%|██████████| 220/220 [00:00<00:00, 12211.30it/s]\n",
            "Processing Entities: 100%|██████████| 220/220 [00:00<00:00, 65405.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRBSA82ijQKo",
        "outputId": "ada088b8-591c-4d70-eff5-1ec821a2ec97"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'entities': [(3421, 3458, 'College Name'), (3381, 3419, 'Degree')],\n",
              " 'text': \"Akhil Yadav Polemaina Hyderabad, Telangana - Email me on Indeed: indeed.com/r/Akhil-Yadav-Polemaina/ f6931801c51c63b1  ● Senior System Engineer at Infosys with 3.2 years of experience in software development and Maintenance. ● Maintained data processing using mainframe technology for multiple front end applications of Walmart Retail Link platform and ensured on-time deliverables. ● Worked on automating the uses cases to reduce manual effort in solving repeating incidents using Service Now orchestration. ● Possess good analytical, logical ability and systematic approach to problem analysis, strong debugging and troubleshooting skills. ● Good exposure to Retail domain.  Willing to relocate to: hyderbad, Telangana  WORK EXPERIENCE  Senior Systems Engineer  Infosys Limited -  Hyderabad, Telangana -  January 2015 to Present  ● Working on all the Major and Minor Enhancement requests as part of Maintenance and Support activities ● Identifying and fixing all the major defects in the applications, perform root cause analysis for production issues ● Being a subject matter expert, involved in multiple Knowledge transfer and knowledge sharing sessions with the client ● Leading a peer group and taking end to end responsibilities for all the critical issues/ enhancements. ● Identifying the use cases which can be automated using Service Now Orchestration ● Creating workflows to automate various tasks which involved manual intervention ● Direct interaction with the client on various business impacting issues on a daily basis ● Setting up Weekly Status Review meetings and Code Review meetings with the client  Senior Systems Engineer  Infosys Limited -  Hyderabad, Telangana -  January 2015 to Present  Team Size # 5 Project Objective: Providing end to end Maintenance and Support activity for data processing of the most critical and important Web portal 'Retail Link' along with over 40 applications used daily by all the Suppliers and Business users of Walmart, the largest retailer in the world. Retail link is a portal which hosts 100's of applications developed across technologies for the suppliers which help them to carry on day-to-day activities right from on boarding to tracking their sales. This involves supporting  https://www.indeed.com/r/Akhil-Yadav-Polemaina/f6931801c51c63b1?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Akhil-Yadav-Polemaina/f6931801c51c63b1?isid=rex-download&ikw=download-top&co=IN   various Decision Support System reports which helps the higher management to take business critical decisions.  Responsibilities: ● Working on all the Major and Minor Enhancement requests as part of Maintenance and Support activities ● Identifying and fixing all the major defects in the applications, perform root cause analysis for production issues ● Being a subject matter expert, involved in multiple Knowledge transfer and knowledge sharing sessions with the client ● Leading a peer group and taking end to end responsibilities for all the critical issues/ enhancements. ● Identifying the use cases which can be automated using Service Now Orchestration ● Creating workflows to automate various tasks which involved manual intervention ● Direct interaction with the client on various business impacting issues on a daily basis ● Setting up Weekly Status Review meetings and Code Review meetings with the client  EDUCATION  Electrical and Electronics Engineering  Anurag College of Engineering (Jntuh)  SKILLS  servicenow (1 year), Mainframe (3 years), cobol (3 years), Jcl (3 years), Teradata (3 years)  ADDITIONAL INFORMATION  Technical Skills • Domain - Retail • Technology - Mainframe (COBOL, JCL, DB2, Teradata), Service now. • Operating System - Mainframe (z/OS) • Database - DB2, SQL, Teradata. • Utilities - FILE-AID, IDCAMS, DFSORT basics, LIBRARIAN, FTP/SFTP, CA-7 basics. • Tools - Query Management Tool (QMF), SQL Assistant, Service now, Remedy.  Key Strengths: ● Effective Communication Skills and Zeal to learn. ● Flexibility and Adaptability. ● Good Leadership Qualities. ● Analytical and Problem Solving Skills.  Achievements: ● Received STAR award for working on various system improvement and automation activities ● Received multiple INSTA awards for my performance in the projects worked\"}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\") # load a new spacy model\n",
        "doc_bin = DocBin()"
      ],
      "metadata": {
        "id": "UxbPrGemWGGp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for training_example  in tqdm(training_data): \n",
        "    text = training_example['text']\n",
        "    labels = training_example['entities']\n",
        "    doc = nlp.make_doc(text) \n",
        "    ents = []\n",
        "    for start, end, label in labels:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        if span is None:\n",
        "            print(\"Skipping entity\")\n",
        "        else:\n",
        "            ents.append(span)\n",
        "    filtered_ents = filter_spans(ents)\n",
        "    doc.ents = filtered_ents \n",
        "    doc_bin.add(doc)\n",
        "\n",
        "doc_bin.to_disk(\"/content/gdrive/MyDrive/ResumeRanker/Models/SPACY3_NER_CLG/training_data.spacy\") # save the docbin object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsf0ma-zWOiw",
        "outputId": "e401f99e-bdac-466a-eb43-53ceff04f6ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 220/220 [00:02<00:00, 108.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/ResumeRanker/Models/SPACY3_NER_CLG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjg6gXoJWjrt",
        "outputId": "b6633bce-0115-46a7-c294-b2d8941e7d95"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ResumeRanker/Models/SPACY3_NER_CLG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ikc0b-0X_om",
        "outputId": "fa3c3231-7fd1-4a6d-994e-245a8fb0bc5a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKch0v5AYuwl",
        "outputId": "f5bea9e3-ae75-458a-f889-a84900058a3a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-07-14 07:17:51,465] [INFO] Set up nlp object from config\n",
            "[2022-07-14 07:17:51,476] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2022-07-14 07:17:51,481] [INFO] Created vocabulary\n",
            "[2022-07-14 07:17:51,482] [INFO] Finished initializing nlp object\n",
            "Downloading: 100% 481/481 [00:00<00:00, 498kB/s]\n",
            "Downloading: 100% 878k/878k [00:00<00:00, 8.44MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 3.93MB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 11.1MB/s]\n",
            "Downloading: 100% 478M/478M [00:07<00:00, 63.6MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-07-14 07:18:18,838] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0       15361.93   1065.44    0.02    0.01    0.16    0.00\n",
            "  2     200      165788.85  54309.04   10.32    5.78   48.36    0.10\n",
            "  5     400        1213.80  30374.72   72.93   75.84   70.23    0.73\n",
            "  7     600         760.87  27004.52   78.63   80.17   77.14    0.79\n",
            " 10     800        1605.75  27049.91   84.46   93.07   77.30    0.84\n",
            " 12    1000         478.45  24690.10   85.02   84.19   85.86    0.85\n",
            " 15    1200         491.57  23560.61   87.29   91.37   83.55    0.87\n",
            " 17    1400         301.26  22894.47   93.79   94.49   93.09    0.94\n",
            " 20    1600         233.08  19973.41   94.30   96.23   92.43    0.94\n",
            " 23    1800         320.49  19564.60   94.35   92.56   96.22    0.94\n",
            " 25    2000          94.07  16303.27   96.65   96.10   97.20    0.97\n",
            " 28    2200         182.00  15378.11   96.75   95.66   97.86    0.97\n",
            " 30    2400          96.32  12376.19   94.71   93.72   95.72    0.95\n",
            " 33    2600         142.94  10446.68   97.28   97.52   97.04    0.97\n",
            " 35    2800        1441.84   8175.51   94.16   91.60   96.88    0.94\n",
            " 38    3000         549.33   5809.57   97.77   98.18   97.37    0.98\n",
            " 41    3200          83.36   4124.90   97.47   96.61   98.36    0.97\n",
            " 43    3400          79.32   2452.79   97.86   98.02   97.70    0.98\n",
            " 46    3600         105.13   1599.46   97.46   97.22   97.70    0.97\n",
            " 48    3800         407.10    874.99   98.60   98.52   98.68    0.99\n",
            "\n",
            "Aborted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"/content/gdrive/MyDrive/ResumeRanker/Models/SPACY3_NER_CLG/model-best\")"
      ],
      "metadata": {
        "id": "p30JB6fDZJwq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in training_data[:2]:\n",
        "    text = i[\"text\"]\n",
        "    print(\"Data :\")\n",
        "    print(text)\n",
        "    doc = nlp(\" \".join(text.split('\\n')))\n",
        "    for ent in doc.ents:\n",
        "        print(f'{ent.label_.upper():{20}} - {ent.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0e9rfA9_4pT",
        "outputId": "8e7035a4-66dd-405b-ada1-11d9ef2d07cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\n",
            "COLLEGE NAME         - B.v.b college of engineering and technology\n",
            "COLLEGE NAME         - Woodbine modern school\n",
            "COLLEGE NAME         - Kendriya Vidyalaya\n",
            "Data :\n",
            "Afreen Jamadar Active member of IIIT Committee in Third year  Sangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6  I wish to use my knowledge, skills and conceptual understanding to create excellent team environments and work consistently achieving organization objectives believes in taking initiative and work to excellence in my work.  WORK EXPERIENCE  Active member of IIIT Committee in Third year  Cisco Networking -  Kanpur, Uttar Pradesh  organized by Techkriti IIT Kanpur and Azure Skynet. PERSONALLITY TRAITS: • Quick learning ability • hard working  EDUCATION  PG-DAC  CDAC ACTS  2017  Bachelor of Engg in Information Technology  Shivaji University Kolhapur -  Kolhapur, Maharashtra  2016  SKILLS  Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)  ADDITIONAL INFORMATION  TECHNICAL SKILLS:  • Programming Languages: C, C++, Java, .net, php. • Web Designing: HTML, XML • Operating Systems: Windows […] Windows Server 2003, Linux. • Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.  https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\n",
            "DEGREE               - PG-DAC\n",
            "COLLEGE NAME         - CDAC ACTS\n",
            "DEGREE               - Bachelor of Engg in Information Technology\n",
            "COLLEGE NAME         - Shivaji University Kolhapur\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "Bhanu Prakash Pebbeti \n",
        "\n",
        "ML/DL Enthusiast | Implementation based learner | Looking for an opportunity to expand my\n",
        "learning, knowledge and skills which help me in achieving greater practical excellence and\n",
        "contribute to the success of the organization. \n",
        "\n",
        "pebbetibhanu2017@gmail.com \n",
        "\n",
        "+91 6303733897 \n",
        "\n",
        "Hyderabad, Telangana, India \n",
        "\n",
        "www.hackerrank.com/bhanuprakash_b12 \n",
        "\n",
        "linkedin.com/in/bhanu-prakash-pebbeti-700b80191 \n",
        "\n",
        "github.com/BhanuPrakashPebbeti \n",
        "\n",
        "EDUCATION \n",
        "\n",
        "ELECTRONICS AND COMMUNICATION\n",
        "ENGINEERING | B.TECH \n",
        "National Institute of Technology Calicut \n",
        "2019 - Present,  \n",
        "\n",
        "CGPA-8.72/10(till 5th sem) \n",
        "\n",
        "SKILLS \n",
        "\n",
        "Python \n",
        "\n",
        "ML \n",
        "\n",
        "AI \n",
        "\n",
        "DL \n",
        "\n",
        "WORK EXPERIENCE \n",
        "\n",
        "INTERMEDIATE \n",
        "Narayana Junior College,Hyderabad \n",
        "2017 - 2019,  \n",
        "\n",
        "Percentage-97.7% \n",
        "\n",
        "Member at AI Club NITC (11/2020 - Present)\n",
        "One of the member at AI Club NITC, aimed at high quality\n",
        "Artiﬁcial Intelligence research and developing Artiﬁcial\n",
        "Intelligence systems for real world applications. \n",
        "\n",
        "SECONDARY HIGH SCHOOL-SSC \n",
        "Shivappa High School,Hyderabad \n",
        "2017,  \n",
        "\n",
        "GPA-9.5/10 \n",
        "\n",
        "Computer Vision Engineer at Intelligent\n",
        "Mobility Labs (06/2021 - Present) \n",
        "Research Lab focused on Self Driving Technology and\n",
        "Autonomous Mobile Robots. \n",
        "\n",
        "PROJECTS \n",
        "\n",
        "Automation  of  Cleaning  Cervical  dataset  using  deep\n",
        "learning techniques (01/2021 - 05/2021) \n",
        "\n",
        "Used Supervised contrastive learning to remove outliers and boost\n",
        "our classiﬁer performance. \n",
        "\n",
        "Multi Task Learning(MTL) for Self Driving Technology\n",
        " (05/2021 - Present)\n",
        "\n",
        "Worked on Perception stack for Indian Road Conditions which\n",
        "includes Semantic segmentation, Depth Estimation and Object\n",
        "detection using MTL. \n",
        "\n",
        "Reinforcement Learning to solve Games\n",
        "\n",
        "Worked on models like Reinforce, Sarsa, Q-Learning, DQN, Deuling\n",
        "DQN to solve games like Balancing Pendulum, CartPole, Lunar\n",
        "Lander from OpenAI Gym and custom made environments like Flappy\n",
        "Bird. \n",
        "\n",
        "Image Generation using VQVAE\n",
        "\n",
        "Used VQVAE to learn discreate representations of the images and\n",
        "then a gpt prior is trained on top of these representations to\n",
        "generate new images. \n",
        "\n",
        "CERTIFICATIONS \n",
        "\n",
        "Applied Data Science With Python\n",
        "Specialization (08/2020)\n",
        "Coursera-University of Michigan \n",
        "\n",
        "Neural Networks and Deep Learning (08/2020)\n",
        "\n",
        "Coursera-deeplearning.ai \n",
        "\n",
        "Python for Everybody Specialization (05/2020)\n",
        "\n",
        "Coursera-University of Michigan \n",
        "\n",
        "LANGUAGES \n",
        "\n",
        "English \n",
        "Fluent \n",
        "\n",
        "Telugu \n",
        "Native \n",
        "\n",
        "Sudoko Solver\n",
        "\n",
        "Application made using python which solves sudoko puzzles with a\n",
        "simple Graphical user interface made using pygame. \n",
        "\n",
        "INTERESTS \n",
        "\n",
        "Reading blogs \n",
        "\n",
        "Playing Sports(cricket) \n",
        "'''"
      ],
      "metadata": {
        "id": "tqn6yj3-B-XD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\" \".join(text.split('\\n')))\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_.upper():{20}} - {ent.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO2jh9cNBvyx",
        "outputId": "c815c8c6-603f-4482-85ef-99b70bbbb9f4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEGREE               - ELECTRONICS AND COMMUNICATION ENGINEERING\n",
            "DEGREE               - | B.TECH\n",
            "COLLEGE NAME         - National Institute of Technology Calicut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install docx2txt\n",
        "!pip install pdfminer"
      ],
      "metadata": {
        "id": "W78Aub2A1eIe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from dateutil import parser\n",
        "import nltk\n",
        "import docx2txt\n",
        "from datetime import datetime\n",
        "from dateutil import relativedelta\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFSyntaxError\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Helper function to extract the plain text from .pdf files\n",
        "    :param pdf_path: path to PDF file to be extracted (remote or local)\n",
        "    :return: iterator of string of extracted text\n",
        "    \"\"\"\n",
        "    if not isinstance(pdf_path, io.BytesIO):\n",
        "        # extract text from local pdf file\n",
        "        with open(pdf_path, \"rb\") as fh:\n",
        "            try:\n",
        "                for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
        "                    resource_manager = PDFResourceManager()\n",
        "                    fake_file_handle = io.StringIO()\n",
        "                    converter = TextConverter(\n",
        "                        resource_manager,\n",
        "                        fake_file_handle,\n",
        "                        # codec=\"utf-8\",\n",
        "                        laparams=LAParams(),\n",
        "                    )\n",
        "                    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "                    page_interpreter.process_page(page)\n",
        "\n",
        "                    text = fake_file_handle.getvalue()\n",
        "                    yield text\n",
        "\n",
        "                    # close open handles\n",
        "                    converter.close()\n",
        "                    fake_file_handle.close()\n",
        "            except PDFSyntaxError:\n",
        "                return\n",
        "    else:\n",
        "        # extract text from remote pdf file\n",
        "        try:\n",
        "            for page in PDFPage.get_pages(\n",
        "                pdf_path, caching=True, check_extractable=True\n",
        "            ):\n",
        "                resource_manager = PDFResourceManager()\n",
        "                fake_file_handle = io.StringIO()\n",
        "                converter = TextConverter(\n",
        "                    resource_manager,\n",
        "                    fake_file_handle,\n",
        "                    # codec=\"utf-8\",\n",
        "                    laparams=LAParams(),\n",
        "                )\n",
        "                page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "                page_interpreter.process_page(page)\n",
        "\n",
        "                text = fake_file_handle.getvalue()\n",
        "                yield text\n",
        "\n",
        "                # close open handles\n",
        "                converter.close()\n",
        "                fake_file_handle.close()\n",
        "        except PDFSyntaxError:\n",
        "            return\n",
        "\n",
        "\n",
        "def get_number_of_pages(file_name):\n",
        "    try:\n",
        "        if isinstance(file_name, io.BytesIO):\n",
        "            # for remote pdf file\n",
        "            count = 0\n",
        "            for page in PDFPage.get_pages(\n",
        "                file_name, caching=True, check_extractable=True\n",
        "            ):\n",
        "                count += 1\n",
        "            return count\n",
        "        else:\n",
        "            # for local pdf file\n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                count = 0\n",
        "                with open(file_name, \"rb\") as fh:\n",
        "                    for page in PDFPage.get_pages(\n",
        "                        fh, caching=True, check_extractable=True\n",
        "                    ):\n",
        "                        count += 1\n",
        "                return count\n",
        "            else:\n",
        "                return None\n",
        "    except PDFSyntaxError:\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_from_docx(doc_path):\n",
        "    \"\"\"\n",
        "    Helper function to extract plain text from .docx files\n",
        "    :param doc_path: path to .docx file to be extracted\n",
        "    :return: string of extracted text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        temp = docx2txt.process(doc_path)\n",
        "        text = [line.replace(\"\\t\", \" \") for line in temp.split(\"\\n\") if line]\n",
        "        return \" \".join(text)\n",
        "    except KeyError:\n",
        "        return \" \"\n",
        "\n",
        "\n",
        "def extract_text(file_path, extension):\n",
        "    \"\"\"\n",
        "    Wrapper function to detect the file extension and call text\n",
        "    extraction function accordingly\n",
        "    :param file_path: path of file of which text is to be extracted\n",
        "    :param extension: extension of file `file_name`\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    if extension == \".pdf\":\n",
        "        for page in extract_text_from_pdf(file_path):\n",
        "            text += \" \" + page\n",
        "    elif extension == \".docx\":\n",
        "        text = extract_text_from_docx(file_path)\n",
        "    elif extension == \".txt\":\n",
        "        with open(file_path, \"r\") as file:\n",
        "            text = file.read().replace(\"\\n\", \"\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "Amfp_g6PCBk7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text(\"/content/vidhi.pdf\",\".pdf\")"
      ],
      "metadata": {
        "id": "fUFEUm671udz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "nccW-6uI15tI",
        "outputId": "eb838137-cf98-438e-fc65-5482e76651a0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' github.com/vidhsss/\\n(+91) 8447202370\\nvidhijain.contact@gmail.com\\n\\nlinkedin.com/in/vidhijain23\\nPortfolio\\nI am highly inclined towards emerging technologies with passion and knowledge towards machine learning, computer vision, data science,\\nand data structures. My out-of-the-box ideas, hardworking nature, and management skills can be an asset.As a sophomore, I started working\\nas a research student which gave me the ability to explore different fields and innovate with deep learning and machine learning.\\n\\nVIDHI JAIN\\n\\nEDUCATION\\nBachelor of Technology, Electrical, Netaji Subhas Institute of Technology, GPA: 8.62/10.00\\nHigher secondary, The Heritage School, Percentage: 93.2/100\\nEXPERIENCE\\nResearch Student\\nNetaji Subhas Institute of Technology , Delhi\\n• Working under Dr. Deepak Kumar Sharma, Department of Information Technology, NSUT, introduced an efficient power saving model for\\n\\nAugust 2019— present\\nApril 2017— May 2019\\n\\nMarch 2021-present\\n\\nsmart homes using time series analysis and modeling\\n\\n• Proposed human activity recognition for autonomous perception in ambient environment using multimodal fusion and bi-CRNN model.\\n\\nSummer Intern\\nAT & T India\\n• Analyzed market segmentation for different IoT industries by forecasting of future market capital to find IoT in healthcare as the target\\n\\nMay 2021 — July 2021\\n\\nindustry\\n\\n• Worked on an IoT healthcare product, blood sugar monitoring smart watch, establishing a time-series based analysis for the customer\\n\\nC++ Programmer\\nCodeSpeedy Technology Private Limited\\n• Blogged computer programming tutorials and articles ( all related to c++ ) for CodeSpeedy.com which is an online coding solution website\\n\\nApril 2020-May 2020\\n\\nto assist programmer.\\n\\nPUBLICATIONS\\n1.\\n\\nJain et al. A Novel Hypertuned Prophet Based Power Saving approach for IOT Enabled Smart Homes. Transactions on Emerging\\nTelecommunications Technologies.\\n\\n2. Sharma et al. Ambient Intelligence based Human Activity Recognition for Autonomous Perception. (under review).\\n\\nPROJECTS\\nGraphical clustering and detection of Earth like Exoplanet\\n• KMeans clustering is applied to generate cluster estimates. The centroids of these clusters are then used as the infill values for missing data\\n• Clusters were created using graphical clustering to detect signatures of Earth-sized exoplanets in noisy data making up millions of light\\n\\ncurves observed by the Kepler space telescope.\\n\\n• Graphical Neural Networks are used for detecting exoplanet candidates in large planetary dataset.\\n\\nHybrid Attention Multi-modal learning for Percieved Mental workload Classification\\n• The proposed approach firstly models each modality through LSTM and Convolution based self attention and then applies bi-modal\\n\\nattention on pairwise modalities and tries to learn the contributing features amongst them.\\n\\n• The results demonstrate the applicability of brain-computer interface using multimodal biosignals and attention networks for quantifying\\n\\ncognitive load in well-validated problem-solving tasks.\\n\\nA Novel Hypertuned Prophet Based Power Saving approach for IOT Enabled Smart Homes\\nMentors : Deepak Kumar Sharma, Department of Information Technology, NSUT and\\nKoyel Datta Gupta, Department of Computer Science, MSIT\\n• This study proposes a home energy management system that analyses a smart home’s net energy consumption based on sensors and\\n\\n•\\n\\nweather conditions and forecasts future energy usage. The introduction of weather conditions has shown to provide significant\\nimprovements to the overall accuracy.\\nIt shows how the Prophet model with additional regressors, logistic growth pattern and parameter hyper tuning is the best fit for\\nforecasting energy utilization in smart homes. The energy analysis and forecast for demand can be optimized for energy efficiency and help\\nstore energy for battery backup during a power outage.\\n\\n\\x0c Ambient Intelligence based Human Activity Recognition for Autonomous Perception\\nMentor: Deepak Kumar Sharma, Department of Information Technology, NSUT\\n• This work proposes an integrated framework that allows navigation through gesture and action recognition in the ambient environment for\\n\\nautonomous system. This is the first work that employs the action recognition of autonomous robots in the AAL environment with high\\naccuracy and controllability.\\n\\n• An encoder based on CRNN is used to learn the spatial-temporal information of auto-fused motion and sensor data through videos,\\n\\nwearable IMUs, and ambient sensors, and a decoder based on Bi-lstm layers is used to finally classify and detect the human gesture and\\naction.\\n\\nEEG based Authentication\\n• A high-security authentication system based on the neurophysiological features of motor skills in a human brain, which can be measured\\n\\nby ElectroEncephaloGraphy (EEG)\\n\\n• Classification based on network of attention based LSTM model and Mobilenet model for EEG signals for high-end authentication.\\nAGRI-AI\\nThis project secured 1st position at IIT Ropar tech fest 2021 among 200 teams.\\n• An end-to-end web application developed to assist farmers in administering their crop quality, control, and foresee selling price\\n• Architectures and frameworks: Html,css, streamlit, mobileNet CNN, Naive bayes Gausssians, Random Forest regressor, Deep neuaral\\n\\nNetwork, Catboost Classifier, Deep Bidirectional LSTM (http://agri-ai.herokuapp.com/)\\n\\nCovid Home Care: An integrated web app for covid patients (health-home.herokuapp.com/)\\n•\\n\\nIt is an all-in-one web app for home quarantined covid patients assisting them\\n– to predict disease severity through random forest classfier with a symptom manager,\\n– access nearby resources and information,\\n– maintain healthy environment through positive news portal and content-base recommendation systems.\\n\\n• Equiped with an AI chatbot providing information and the latest tweet based on user demand (in case of emergencies) with queries.\\n\\nIt handles login and log out of users, updation of profile and posting, updating and deleting the tweet\\n\\nTwitter Clone\\n• A Twitter clone implementation using Django and bootstrap having a login page, dashboard, and profile.\\n•\\nObject Detection for sweeping robot\\n• A single shot detection (SSD) algorithm called You Only Look Once “YOLO” is used on images at a specific angle( from view of sweeping\\n\\n• The image processing was done with OpenCV and the processed images were used to train the last fully connected layers of the network\\n\\nusing Python’s Keras package with Darknet and Tensorflow as backends.\\n\\nrobots).\\n\\nPOSITION OF RESPONISIBILITY\\nExecutive Director\\nEnactus NSUT\\n• As a part of team, launched the Led bulbs which consisted of a high-energy LED, a solar panel connected to the battery, and a reused plastic\\n\\nAugust 2019 — present\\n\\nbottle half-filled with water for slum regions experiencing frequent power cuts\\n\\n• Currently, developing an e-commerce web app for artisans who lost work due to covid to get an audience for shows, sell products, and\\nconduct workshops. The web app will showcase all the services provided by the community with a calendar for the upcoming events.\\n\\nMachine Learning Engineer\\nTeam Kalpana\\n• worked on real-time satellite tracking and fetching of data from it with data analysis on various parameters . The plots could be observed\\n\\nOct 2020 - March 2021\\n\\nand analysed through a tkinter based GUI Dashboard\\n\\nTECHNICAL SKILLS\\nProgramming Languages\\n• C++\\n• Python\\n• Matlab\\n• VHDL\\n• Data Structures and Algorithms\\n\\nTools and Technologies\\nProficient\\n• Machine Learning\\n• Data Analysis\\n• Deep Learning Frameworks: Tensorflow, Keras\\n\\nExperienced\\n• Computer Vision and Natural language processing\\n• Web Frameworks: Flask, Django\\n• Google Cloud AI platform\\n\\nAWARDS\\n• Secured 1st position at IIT Ropar tech fest 2021 in a 3 Day National level virtual Hackathon among 150 teams\\n• Won 1000 USD in Ford Fund COVID-19 Global Challenge\\n• Winner of Mars settlement Project at Mars Global Teen Summit, Nasa space centre, Houston\\n\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\" \".join(text.split('\\n')))\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_.upper():{20}} - {ent.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCSSBQIh2yTt",
        "outputId": "4ba61b62-5737-4818-e057-26316017feb5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEGREE               - Bachelor of Technology, Electrical\n",
            "COLLEGE NAME         - Netaji Subhas Institute of Technology\n",
            "COLLEGE NAME         - Netaji Subhas Institute of Technology\n",
            "DEGREE               - e-commerce\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sOfJiHqW25vl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}